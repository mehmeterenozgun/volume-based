{
 "cells": [
  {
   "cell_type": "code",
   "id": "34d4292b1b588460",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Imports \n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_nn_ops import LeakyRelu\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_stationarity(series):\n",
    "    adf_result = adfuller(series)\n",
    "    print(\"ADF Statistic:\", adf_result[0])\n",
    "    print(\"p-value:\", adf_result[1])\n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"The series is stationary.\")\n",
    "    else:\n",
    "        print(\"The series is not stationary.\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def window_generator(data, feature_columns, target_column, input_size, output_size, stride):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    data = data.sort_index()\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    scaled_features = feature_scaler.fit_transform(data[feature_columns])\n",
    "    scaled_target = target_scaler.fit_transform(data[target_column])\n",
    "\n",
    "    for start in range(0, len(data) - input_size - output_size + 1, stride):\n",
    "        end_input = start + input_size\n",
    "        end_output = end_input + output_size\n",
    "\n",
    "        X.append(scaled_features[start:end_input])\n",
    "        y.append(scaled_target[end_input:end_output])\n",
    "\n",
    "    return np.array(X), np.array(y), feature_scaler, target_scaler\n",
    "\n",
    "\n",
    "\n",
    "def test_window_generator(test_data, feature_columns, target_column, input_size, output_size, stride, feature_scaler, target_scaler):\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    test_data = test_data.sort_index()\n",
    "\n",
    "    scaled_features = feature_scaler.transform(test_data[feature_columns])\n",
    "    scaled_target = target_scaler.transform(test_data[target_column])\n",
    "\n",
    "    for start in range(0, len(test_data) - input_size - output_size + 1, stride):\n",
    "        end_input = start + input_size\n",
    "        end_output = end_input + output_size\n",
    "\n",
    "        X_test.append(scaled_features[start:end_input])\n",
    "        y_test.append(scaled_target[end_input:end_output])\n",
    "\n",
    "    return np.array(X_test), np.array(y_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e879447c99b13ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"tickers.txt\", \"r\") as file:\n",
    "    tickers = [line.strip() for line in file]\n",
    "\n",
    "output_file = \"sp500_data.csv\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Ticker,Date, Volume,Price_Change\\n\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        print(f\"Fetching data for {ticker}...\")\n",
    "        stock = yf.Ticker(ticker)\n",
    "        index = stock.history(start=\"2010-01-01\", end=\"2020-01-01\")\n",
    "\n",
    "        if index.empty:\n",
    "            print(f\"No data for {ticker}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        expected_days = 252 * 9  # 10 years of trading data\n",
    "        if len(index) < expected_days:\n",
    "            print(f\"Incomplete data for {ticker} ({len(index)} days). Skipping...\")\n",
    "            continue\n",
    "\n",
    "        index['Price_Change'] = index['Close'].pct_change()\n",
    "\n",
    "        index.reset_index(inplace=True)\n",
    "        index['Ticker'] = ticker\n",
    "        index[['Ticker', 'Date', 'Volume', 'Price_Change']].to_csv(\n",
    "            output_file, mode='a', header=False, index=False\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {ticker}: {e}\")\n",
    "print(f\"Data collection complete! Saved to {output_file}.\")"
   ],
   "id": "cdcfe2e1283cd21b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clean_data = pd.read_csv(\"sp500_data.csv\")\n",
    "print(clean_data.shape)\n",
    "missing_summary = clean_data.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Get unique tickers from the 'Ticker' column\n",
    "unique_tickers = clean_data['Ticker'].unique()\n",
    "\n",
    "# Count the number of unique tickers\n",
    "print(f\"Number of unique tickers: {len(unique_tickers)}\")\n",
    "\n",
    "#X = clean_data.drop(columns=['Price_Change'])\n",
    "#y = clean_data['Price_Change']\n",
    "#print(X.shape, y.shape)\n",
    "#print(X.head())\n",
    "#print(y.head())"
   ],
   "id": "283de1804598b651",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Download data\n",
    "data = yf.download(\"^GSPC\", start=\"2010-01-01\", end=\"2020-01-01\")\n",
    "\n",
    "# Select relevant columns\n",
    "index_data = data[[\"Volume\", \"Close\"]]\n",
    "\n",
    "# Calculate Price Change\n",
    "index_data['Price_Change'] = index_data['Close'].pct_change()\n",
    "\n",
    "# Reset the MultiIndex columns\n",
    "index_data.columns = ['Volume', 'Close', 'Price_Change']\n",
    "\n",
    "# Check for rows with NaN values\n",
    "print(\"Rows with NaN values:\")\n",
    "print(index_data[index_data.isna().any(axis=1)])\n",
    "\n",
    "# Drop rows with NaN values\n",
    "index_data = index_data.dropna(how='any')  # Drops rows where any column has NaN\n",
    "\n",
    "# Save cleaned data to CSV\n",
    "index_data.to_csv(\"sp500_index_volume.csv\", mode='w', header=True, index=True)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"Cleaned Data:\")\n",
    "print(index_data.head())"
   ],
   "id": "d1af2748095a3955",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def format_large_values(x, pos):\n",
    "    return f'{int(x):,}$'\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12), sharex=True)\n",
    "\n",
    "ax1.plot(index_data.index, index_data['Volume'].values, label=\"Volume\", linewidth=0.8, color='blue')\n",
    "ax1.set_title(\"S&P 500 Index Volume (2010-2020)\", fontsize=14)\n",
    "ax1.set_ylabel(\"Volume\", fontsize=12)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax1.yaxis.set_major_formatter(FuncFormatter(format_large_values))\n",
    "ax1.axhline(y=index_data['Volume'].values.mean(), color='red', linestyle='--', label=\"Mean Volume Over Period\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(index_data.index, index_data['Close'].values, label=\"Close Price\", linewidth=0.8, color='green')\n",
    "ax2.axhline(y=index_data['Close'].values.mean(), color='red', linestyle='--', label=\"Mean Volume Over Period\")\n",
    "ax2.set_title(\"S&P 500 Index Close Price (2010-2020)\", fontsize=14)\n",
    "ax2.set_xlabel(\"Date\", fontsize=12)\n",
    "ax2.set_ylabel(\"Close Price\", fontsize=12)\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3f30d6fceb2987a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index_data.shape, clean_data.shape"
   ],
   "id": "dc7557f91774101",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ticker_data = clean_data.pivot(index='Date', columns='Ticker', values=[' Volume', 'Price_Change'])\n",
    "\n",
    "ticker_data.columns = ['_'.join(col).strip() for col in ticker_data.columns.values]\n",
    "\n",
    "ticker_data = ticker_data.reset_index()\n",
    "\n",
    "ticker_data['Date'] = pd.to_datetime(ticker_data['Date'], errors='coerce')  # Handle malformed dates gracefully\n",
    "\n",
    "\n",
    "ticker_data = ticker_data.set_index('Date')\n",
    "ticker_data.head(2)\n",
    "\n",
    "ticker_data = ticker_data[~ticker_data.index.isna()]\n",
    "\n",
    "ticker_data.to_csv(\"ticker_data.csv\")\n",
    "\n",
    "ticker_data.index = pd.to_datetime(ticker_data.index, utc=True)\n",
    "ticker_data.index = ticker_data.index.date\n",
    "ticker_data.dropna(how='any', inplace=True)\n"
   ],
   "metadata": {},
   "id": "e6c892ff72117fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(index_data[\"Volume\"], index_data[\"Price_Change\"], alpha=0.5)\n",
    "plt.title(\"Volume vs. Price Change (%)\", fontsize=14)\n",
    "plt.xlabel(\"Volume\", fontsize=12)\n",
    "plt.ylabel(\"Price Change (%)\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ],
   "id": "a15b2e305548d915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "High Volume and High Price Change:\n",
    "\n",
    "Outliers on the top-right or bottom-right of the graph (large volume with significant price changes) could indicate market-moving events.\n",
    "Market Stability:\n",
    "\n",
    "The concentration of points near the origin suggests that most days see stable prices and moderate trading activity.\n",
    "Event-Driven Volatility:\n",
    "\n",
    "The outliers (large price changes or unusually high volumes) may help you identify event-driven periods that warrant further investigation.\n"
   ],
   "id": "c69f8c3a2796911c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index_datas = pd.read_csv('sp500_index_volume.csv')\n",
    "        \n",
    "check_stationarity(index_datas['Volume'].values)\n",
    "check_stationarity(index_datas['Price_Change'].values)"
   ],
   "id": "76a504d756054d65",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X,y,feature_scaler, target_scaler = window_generator(ticker_data,ticker_data.columns ,['Price_Change_AAPL'],60,3,1)",
   "metadata": {
    "collapsed": false
   },
   "id": "589f5dcff3ecd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5658359df59fe8c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Input shape (X):\", X.shape)  # Expected: (813, 60, 1)\n",
    "print(\"Target shape (y):\", y.shape) # Expected: (813, 20, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54af8e7bdb818e06",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "file_path = 'sp500_index_volume.csv'\n",
    "index_volume = pd.read_csv(file_path)\n",
    "\n",
    "index_volume['Date'] = pd.to_datetime(index_volume['Date'])\n",
    "index_volume.set_index('Date', inplace=True)\n",
    "\n",
    "biweekly_data = index_volume['Volume'].resample('1W').sum()\n",
    "\n",
    "train_data = biweekly_data['2010-01-01':'2015-01-01']\n",
    "test_data = biweekly_data['2015-01-01':'2019-12-31']\n",
    "\n",
    "fixed_order = (1, 0, 0)\n",
    "initial_model = ARIMA(train_data, order=fixed_order)\n",
    "fitted_model = initial_model.fit()\n",
    "\n",
    "predictions_recursive = []\n",
    "train_data_dynamic = train_data.copy()\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    forecast = fitted_model.get_forecast(steps=1)\n",
    "    predicted_value = forecast.predicted_mean.iloc[0]\n",
    "    predictions_recursive.append(predicted_value)\n",
    "\n",
    "    # Update training data with the actual value from the test set\n",
    "    new_data = test_data.iloc[i:i+1]\n",
    "    train_data_dynamic = pd.concat([train_data_dynamic, new_data])\n",
    "\n",
    "    # Refit the model using the updated training data\n",
    "    fitted_model = ARIMA(train_data_dynamic, order=fixed_order).fit()\n",
    "\n",
    "# Convert predictions to a pandas Series\n",
    "predictions_recursive = pd.Series(predictions_recursive, index=test_data.index)\n",
    "\n",
    "\n",
    "# Evaluate predictions\n",
    "mse_recursive = mean_squared_error(test_data, predictions_recursive)\n",
    "print(f\"Mean Squared Error (Recursive ARMA): {mse_recursive}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data, label=\"Training Data\")\n",
    "plt.plot(test_data, label=\"Actual Test Data\", color=\"green\")\n",
    "plt.plot(predictions_recursive, label=\"Recursive Predictions\", color=\"red\")\n",
    "plt.title(\"Recursive ARMA Model Predictions (Bi-Weekly Volume)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "index_volume['Close'].plot(title=\"Price Change Over Time\", color='blue', label='Price Change')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f72b3dcb1ab1954a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions_recursive.pct_change()\n",
   "id": "97d3b0afcf2801da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed, Input, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(5, 882)),  # Match input shape with your data\n",
    "\n",
    "    # First LSTM layer\n",
    "    LSTM(25, activation='tanh', return_sequences=True),\n",
    "    Dropout(0.2),  # Regularization\n",
    "\n",
    "    # Second LSTM layer\n",
    "    LSTM(10, activation='tanh', return_sequences=False),\n",
    "    Dropout(0.2),  # Regularization\n",
    "\n",
    "    # Fully connected output layer\n",
    "    Dense(2, activation='linear')  # Output layer for 2 features\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['MAE'])\n",
    "model.summary()"
   ],
   "id": "7470e6e6ab6892c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ticker_data.columns.to_list()\n",
    "\n",
    "# Example column names\n",
    "columns = ticker_data.columns.to_list()\n",
    "\n",
    "# Extract unique tickers from column names\n",
    "tickers = list(set(col.split('_')[1] for col in columns))\n",
    "\n",
    "random_tickers = random.sample(tickers, 2)\n",
    "\n",
    "X_columns = [f\"Volume_{ticker}\" for ticker in columns] + [f\"Price_Change_{ticker}\" for ticker in columns]\n",
    "y_columns = [f\"Price_Change_{ticker}\" for ticker in random_tickers]\n",
    "\n",
    "ticker_data.index = pd.to_datetime(ticker_data.index)\n",
    "\n",
    "train_data = ticker_data.loc['2010-01-01':'2018-01-01']\n",
    "test_data = ticker_data.loc['2018-01-01':'2020-01-01']\n",
    "X_train, y_train , feature_scaler, target_scaler = window_generator(ticker_data['2010-01-01' : '2018-01-01'], columns, y_columns, 5, 1, 1)\n",
    "X_test , y_test = test_window_generator(test_data, columns, y_columns, 5, 1, 1, feature_scaler, target_scaler)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ],
   "id": "de6fdba0762134c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[lr_scheduler])"
   ],
   "id": "bf00f92586fec5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predicting y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_test to match y_pred if needed\n",
    "y_test_reshaped = y_test.reshape(y_test.shape[0], -1)  # Flatten to (473, 2)\n",
    "\n",
    "# Inverse transform predictions and ground truth\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_reshaped)\n",
    "\n",
    "# Evaluate results (e.g., using Mean Squared Error)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Flatten both arrays for MSE if they are still multidimensional\n",
    "mse = mean_squared_error(y_test_original.flatten(), y_pred_original.flatten())\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Optionally, visualize predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(y_test_original.shape[1]):  # Iterate over target columns\n",
    "    plt.plot(y_test_original[:, i], label=f'True Values - Target {i + 1}')\n",
    "    plt.plot(y_pred_original[:, i], label=f'Predicted Values - Target {i + 1}', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "predictions_df = pd.DataFrame(y_pred_original, columns=[\"Ticker 0\", \"Ticker 1\"])\n",
    "targets_df = pd.DataFrame(y_test_original, columns=[\"Ticker 0\", \"Ticker 1\"])\n",
    "\n"
   ],
   "id": "229e1e25b12301da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert predictions and targets to DataFrames if not already\n",
    "predictions_df = pd.DataFrame(y_pred_original, columns=[\"Ticker 0\", \"Ticker 1\"])\n",
    "targets_df = pd.DataFrame(y_test_original, columns=[\"Ticker 0\", \"Ticker 1\"])\n",
    "\n",
    "# Plot for Ticker 0\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(targets_df[\"Ticker 0\"], label=\"Actual Values - Ticker 0\", color=\"blue\")\n",
    "plt.plot(predictions_df[\"Ticker 0\"], label=\"Predicted Values - Ticker 0\", linestyle=\"dashed\", color=\"orange\")\n",
    "plt.title(\"Ticker 0: Actual vs Predicted\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Ticker 1\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(targets_df[\"Ticker 1\"], label=\"Actual Values - Ticker 1\", color=\"blue\")\n",
    "plt.plot(predictions_df[\"Ticker 1\"], label=\"Predicted Values - Ticker 1\", linestyle=\"dashed\", color=\"orange\")\n",
    "plt.title(\"Ticker 1: Actual vs Predicted\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "609716d71d9a0065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_pred_original",
   "id": "e9fba4151a548698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metrics from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mape = history.history['MAE']\n",
    "val_mape = history.history['val_MAE']\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(val_loss, label='Validation Loss', marker='x')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot MAPE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_mape, label='Training MAE', marker='o')\n",
    "plt.plot(val_mape, label='Validation MAE', marker='x')\n",
    "plt.title('Training and Validation MAE over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "97b558fb7c922299",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ad59a9adac9b7701",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
